{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26328c42",
   "metadata": {},
   "source": [
    "# Train a score-based diffusion model to generate a 1d distribution.\n",
    "\n",
    "We use **seaborn** to plot probability density function. \n",
    "\n",
    "For installation and introduction, see:\n",
    "\n",
    "https://seaborn.pydata.org/\n",
    "\n",
    "https://seaborn.pydata.org/installing.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd14c4",
   "metadata": {},
   "source": [
    "## Step 1, generate data.\n",
    "\n",
    "We generate data by sampling a Brownian dynamics. \n",
    "\n",
    "We simply reuse the potential $V$ we studied before for Markov state models and learning eigenfunctions.\n",
    "\n",
    "### 1.1. functions that define potential and sampling, and also set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential V, one-dimensional\n",
    "def V(x):\n",
    "    y1 = x**8\n",
    "    y2 = 0.8 * np.exp(-80 * x**2)\n",
    "    y3 = 0.55 * np.exp(-80 * (x-0.5)**2)\n",
    "    y4 = 0.3 * np.exp(-80 * (x+0.5)**2)\n",
    "\n",
    "    y = 2 * (y1 + y2 + y3 + y4)\n",
    "\n",
    "    return y\n",
    "\n",
    "# gradient of V\n",
    "def gradV(x):\n",
    "    y1 = 8 * x**7 \n",
    "    y2 = - 0.8 * 160 * x * np.exp(-80 * x**2)\n",
    "    y3 = - 0.55 * 160 * (x - 0.5) * np.exp(-80 * (x-0.5)**2) \n",
    "    y4 = - 0.3 * 160 * (x + 0.5) * np.exp(-80 * (x+0.5)**2)\n",
    "\n",
    "    y = 2 * (y1 + y2 + y3 + y4)\n",
    "\n",
    "    return y\n",
    "\n",
    "# sample the SDE using Euler-Maruyama scheme\n",
    "def sample(beta=1.0, dt=0.001, N=10000, seed=42):\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    X = 0.0\n",
    "    traj = []\n",
    "    tlist = []\n",
    "    for i in range(N):\n",
    "        traj.append(X)\n",
    "        tlist.append(dt*i)        \n",
    "        b = rng.normal()\n",
    "        X = X - gradV(X) * dt + np.sqrt(2 * dt/beta) * b\n",
    "\n",
    "    return np.array(tlist), np.array(traj)  \n",
    "\n",
    "# coefficient in SDE\n",
    "beta = 2.0\n",
    "# step-size \n",
    "dt = 0.005\n",
    "# number of sampling steps \n",
    "N = 10000\n",
    "# range of the domain \n",
    "xmin, xmax = -1.0, 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ff126",
   "metadata": {},
   "source": [
    "### 1.2 sample the SDE and display the trajectory \n",
    "\n",
    "**dataset** contains the training data we will use later.\n",
    "\n",
    "From the figure on the right, we see that our target density has 4 modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling SDE\n",
    "tvec, dataset = sample(beta, dt=dt, N=N)\n",
    "\n",
    "# show how many states are sampled\n",
    "print ('dataset has %d states.\\n' % dataset.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "# plot trajectory vs time\n",
    "ax.plot(tvec, dataset, alpha=0.7)\n",
    "ax.set_ylim([xmin, xmax])\n",
    "ax.set_xlabel(r'time')\n",
    "ax.set_ylabel(r'x')\n",
    "ax.set_title('trajectory')\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "# plot empirical density of the data\n",
    "ax1.hist(dataset, 50, density=True)\n",
    "\n",
    "ax1.set_title('impirical density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2977e",
   "metadata": {},
   "source": [
    "## Step 2, define forward and backward process \n",
    "\n",
    "\n",
    "#### VESDE \n",
    "\n",
    "Given $0 < \\eta_{\\min}\\le \\eta_{\\max}$, consider\n",
    "$$dX_t = \\sqrt{\\beta(t)} dB_t,$$  where \n",
    "  \\begin{equation}\n",
    "      \\beta(t) = \\frac{2\\eta^2_{\\mathrm{min}}}{T}\n",
    "      \\Big(\\frac{\\eta_{\\mathrm{max}}}{\\eta_{\\mathrm{min}}}\\Big)^{2t/T}\n",
    "      \\ln\\Big(\\frac{\\eta_{\\mathrm{max}}}{\\eta_{\\mathrm{min}}}\\Big) , \\quad t \\in [0,T]\\,.\n",
    "  \\end{equation}\n",
    "The solution is  \n",
    "  \\begin{equation}\n",
    "     X_t = X_0 + \\int_0^t \\sqrt{\\beta(s)} dB_s \n",
    "     \\end{equation}\n",
    "\n",
    "1. When $X_0=x_0$ is fixed, then \n",
    "  \\begin{equation*}\n",
    "     X_t \\sim \\mathcal{N}\\Big(x_0, \\eta^2(t)\\mathbf{1}_d\\Big)\\,,\\quad\n",
    "\\mathrm{where}\\quad \n",
    "    \\eta(t) = \\Big(\\int_0^t \\beta(s) ds\\Big)^{\\frac{1}{2}} = \n",
    "    \\eta_{\\mathrm{min}}\n",
    "      \\sqrt{\\Big(\\frac{\\eta_{\\mathrm{max}}}{\\eta_{\\mathrm{min}}}\\Big)^{2t/T} -1}.\n",
    "  \\end{equation*}\n",
    "\n",
    "2. When $X_0\\sim \\mathcal{N}(x_0, \\eta^2_{\\min}\\mathbf{1}_d)$, \n",
    "  \\begin{equation*}\n",
    "    X_t = (x_0 + \\eta_{\\min} z\\big) + \\int_0^t\\sqrt{\\beta(s)} dB_s \\sim \\mathcal{N}(x_0, \\tilde{\\eta}^2(t)\\mathbf{1}_d)\n",
    "  \\end{equation*}\n",
    "  where $z\\sim \\mathcal{N}(0,\\mathrm{I}_d)$ and \n",
    "  \\begin{equation*}\n",
    "    \\tilde{\\eta}(t) = \\sqrt{\\eta^2(t) + \\eta^2_{\\min}} =  \\eta_{\\mathrm{min}} \\Big(\\frac{\\eta_{\\mathrm{max}}}{\\eta_{\\mathrm{min}}}\\Big)^{t/T}.\n",
    "  \\end{equation*}\n",
    "  \n",
    "In particular, we have \n",
    "\\begin{equation*}\n",
    "\\tilde{\\eta}(0) = \\eta_{\\min}, \\quad \\tilde{\\eta}(T) = \\eta_{\\max}.\n",
    "\\end{equation*}\n",
    "\n",
    "As for prior, we use \n",
    "\\begin{equation*}\n",
    "\\mathcal{N}(0, \\eta^2 _{\\max}\\mathbf{1}_d)\n",
    "\\end{equation*}\n",
    "\n",
    "#### Backward process \n",
    "\n",
    "for a general SDE\n",
    "\\begin{equation}\n",
    "    dX_t = f(X_t, t)\\,dt + \\sigma(t) dB_t\\,, \\quad t \\in [0,T]\\,,    \n",
    "\\end{equation}\n",
    "the backward process is\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    dY_t =& \\Big(- f(Y_t, T-t) + \\sigma^2(T-t) \\nabla \\ln p(Y_t,T-t)\\Big)\\,dt + \\sigma(T-t) dB_t\\,,  \\\\\n",
    "    Y_0 \\sim& p(\\cdot, T)\\,.\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "We assume that $p(\\cdot, T)$ is close to a prior that is easy to sample from, and we sample $Y_0$ from prior (instead of $p(\\cdot,T)$) when simulating the backward process.\n",
    "\n",
    "We implement the VESDE in a class, which contains:\n",
    "\n",
    "1. **beta_sqrt**: compute $\\sqrt{\\beta(t)}$: \n",
    "\n",
    "2. **marginal_std**: compute $\\tilde{\\eta}(t)$: \n",
    "\n",
    "3. **prior**: sample from prior $\\mathcal{N}(0, \\eta^2 _{\\max}\\mathbf{1}_d)$\n",
    "\n",
    "4. **forward_sampling**: sample forward process $dX_t = \\sqrt{\\beta(t)} dB_t$\n",
    "\n",
    "5. **backward_sampling**: sample backward process \n",
    "$dY_t =  \\beta(T-t) \\nabla\\ln p(Y_t, T-t)dt + \\sqrt{\\beta(T-t)} dB_t$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VESDE: \n",
    "    def __init__(self, eta_min, eta_max, dim=1, T=1):\n",
    "\n",
    "        self.T = T\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "      \n",
    "    def beta_sqrt(self, t):\n",
    "\n",
    "        sigma = self.eta_min * (self.eta_max/self.eta_min) ** (t/self.T) \n",
    "        \n",
    "        # square root of \\beta(t)\n",
    "        ret = sigma  * torch.sqrt(1.0 / self.T \\\n",
    "                                  * torch.tensor(2 * (math.log(self.eta_max) - math.log(self.eta_min))))\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    def marginal_std(self, X, t):\n",
    "        # standard deviation of Gaussian density at time t\n",
    "        # this is \\tilde{\\eta}(t) \n",
    "        std = self.eta_min * (self.eta_max/self.eta_min) ** (t/self.T) \n",
    "        return std \n",
    "\n",
    "    def prior(self, M):\n",
    "        # Gaussian N(0, \\eta^2_{max})\n",
    "        return torch.randn(M).reshape(-1, self.dim) * self.eta_max\n",
    "    \n",
    "    # sample the forward process\n",
    "    def forward_sampling(self, X0, N=100):\n",
    "        \n",
    "        if torch.is_tensor(X0) is False:\n",
    "            X = torch.tensor(X0).reshape(-1, self.dim)\n",
    "        else :\n",
    "            X = X0.reshape(-1, self.dim)\n",
    "            \n",
    "        traj = [X]\n",
    "        delta_t = self.T / N\n",
    "\n",
    "        for i in range(N):\n",
    "\n",
    "            b = torch.randn_like(X)\n",
    "\n",
    "            t = i * delta_t * torch.ones(X.shape)\n",
    "            \n",
    "            diffusion_coeff = self.beta_sqrt(t)\n",
    "\n",
    "            X = X + diffusion_coeff * math.sqrt(delta_t) * b\n",
    "\n",
    "            traj.append(X)\n",
    "\n",
    "        return torch.stack(traj)\n",
    "\n",
    "    # sample the backward SDE \n",
    "    def backward_sampling(self, X0, model, N=100): \n",
    "        \n",
    "        # change input to torch tensor if necessary, and reshape the tensor\n",
    "        if torch.is_tensor(X0) is False:\n",
    "            X = torch.tensor(X0).reshape(-1, self.dim)\n",
    "        else :           \n",
    "            X = X0.reshape(-1, self.dim)\n",
    "            \n",
    "        traj = [X]\n",
    "        delta_t = self.T / N\n",
    "\n",
    "        for i in range(N):\n",
    "            \n",
    "            b = torch.randn_like(X)\n",
    "            \n",
    "            # reverse the time\n",
    "            t = self.T - i * delta_t * torch.ones_like(X)\n",
    "            \n",
    "            # evaluate the score\n",
    "            score = model(X, t)\n",
    "            \n",
    "            diffusion_coeff = self.beta_sqrt(t)\n",
    "\n",
    "            X = X + (diffusion_coeff**2 * score) * delta_t + math.sqrt(delta_t) * diffusion_coeff * b\n",
    "\n",
    "            traj.append(X)\n",
    "\n",
    "        return torch.stack(traj) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de2253",
   "metadata": {},
   "source": [
    "### specify parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597b1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time interval is [0,T]\n",
    "T = 1\n",
    "N = 500\n",
    "# step-size\n",
    "dt = T/N\n",
    "eta_min = 0.03\n",
    "eta_max = 2\n",
    "\n",
    "sde = VESDE(eta_min, eta_max, dim=1, T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a045f0",
   "metadata": {},
   "source": [
    "### We can simulate the forward process starting from dataset.\n",
    "\n",
    "1. Dataset contains M=10000 states. \n",
    "\n",
    "2. For each state, we generate a trajectory in N=500 steps (each trajectory contains N+1 states).  \n",
    "\n",
    "3. The dimension is dim=1.\n",
    "\n",
    "In the end, we get a tensor that contains M trajectories. \n",
    "\n",
    "The shape of tensor is $[M,N,dim]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the dataset to PyTorch tensor\n",
    "dataset = torch.tensor(dataset, dtype=torch.float32).reshape(-1,1)\n",
    "dataset = torch.randn(5000,2)\n",
    "\n",
    "# sampling forward process on [0,T] \n",
    "forward_traj_set = sde.forward_sampling(dataset, N=N).detach().numpy()\n",
    "\n",
    "# number of states in dataset\n",
    "print (dataset.shape)\n",
    "\n",
    "# trajectory set contains M trajectories. Each has N+1 states of dimension dim=1.\n",
    "print (forward_traj_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee7b3e",
   "metadata": {},
   "source": [
    "### Using the trajectories of forward process, we plot the probability densities $p(x,t)$ at different time $t$.\n",
    "\n",
    "We use the function seaborn.kdeplot for density estimation.\n",
    "\n",
    "See: https://seaborn.pydata.org/generated/seaborn.kdeplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f688d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "\n",
    "# time at which to plot probability densities \n",
    "t_list = [0, 0.5, 0.7, 1.0] * T\n",
    "color_list = ['b', 'y', 'k', 'r', 'gray']\n",
    "\n",
    "# plot densities for each time t in t_list\n",
    "for i, t in enumerate(t_list):\n",
    "    \n",
    "    # the index corresponding to time t\n",
    "    t_idx = int (t / dt)\n",
    "    \n",
    "    # select states at time t from all trajectories, and change it to 1d tensor\n",
    "    traj = forward_traj_set[t_idx, :, :].flatten()\n",
    "    # estimate the density \n",
    "    sns.kdeplot(traj, ax=ax, label='t=%.2f' % t, c=color_list[i])\n",
    "\n",
    "# get samples from prior distribution    \n",
    "X_prior = sde.prior(20000).flatten()\n",
    "\n",
    "# verify that the density p(x,T) is close to the prior (solid and dashed lines in red)\n",
    "sns.kdeplot(X_prior, ax=ax, label='prior', c='r', linestyle='--')\n",
    "    \n",
    "plt.legend()\n",
    "ax.set_xlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfa961",
   "metadata": {},
   "source": [
    "### Define a neural network to model the score function $u$\n",
    "\n",
    "In this example, $u: \\mathbb{R} \\times [0,T] \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "Therefore, input dimension is 2 and output dimension is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyScore(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 50), \n",
    "            nn.Tanh(),                      \n",
    "            nn.Linear(50, 50),             \n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(50, 1), \n",
    "       )\n",
    "        \n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        # combine x and t into one tensor    \n",
    "        state = torch.cat((x, t), dim=1)\n",
    "        \n",
    "        # pass input to the network\n",
    "        output = self.net(state)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "model = MyScore()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863b07e",
   "metadata": {},
   "source": [
    "### display the (untrained) score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2, 2, 100)\n",
    "t = torch.linspace(0, T, 100)\n",
    "xv, tv = torch.meshgrid(x, t, indexing='ij')\n",
    "\n",
    "score_xt = model(xv.reshape(-1, 1), tv.reshape(-1,1)).reshape(100,100)\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "im = ax.pcolormesh(xv.numpy(), tv.numpy(), score_xt.detach().numpy(), cmap='coolwarm',shading='auto')\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=1.0)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "ax.set_xlabel(r'x',fontsize=20)\n",
    "ax.set_ylabel(r't',fontsize=20)\n",
    "ax.set_title('score function',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb5649e",
   "metadata": {},
   "source": [
    "### Let's start learning the score function.\n",
    "\n",
    "#### general result\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\begin{aligned}\n",
    "      & \\mathbb{E}_{t\\sim U([0,T])} \\mathbb{E}_{x\\sim p(\\cdot,t)}\n",
    "      \\Big[\\frac{1}{2}\\big|u(x,t) - \\nabla \\ln p(x,t)\\big|^2 w(t)\\Big] \\\\\n",
    "    =&  \\mathbb{E}_{t\\sim U([0,T])} \\mathbb{E}_{x_0\\sim p_0}\n",
    "      \\mathbb{E}_{x\\sim p(\\cdot,t|x_0)} \\Big[\\frac{1}{2}\\big|u(x,t) - \\nabla \\ln\n",
    "      p(x,t|x_0)\\big|^2 w(t)\\Big] + C_2  \\\\\n",
    "      =&\n",
    "\\mathbb{E}_{t\\sim U([0,T])} \\mathbb{E}_{x_0\\sim p_0}\n",
    "      \\mathbb{E}_{x\\sim p(\\cdot,t|x_0)}\\Big[ \\Big(\\frac{1}{2} |u(x,t)|^2 - u(x,t)\n",
    "      \\cdot \\nabla \\ln p(x,t|x_0)\\Big) w(t)\\Big] + C_3,\n",
    "    \\end{aligned}\n",
    "  \\end{equation}\n",
    "  \n",
    "#### practical loss with VESDE \n",
    "\n",
    "1. Instead of $p(x,t|x_0)$, we use \n",
    "$$\\tilde{p}(x,t|x_0) = \\big(2\\pi \\tilde{\\eta}^2(t)\\big)^{-\\frac{d}{2}} \\mathrm{e}^{-\\frac{1}{2} |x-x_0|^2/ \\tilde{\\eta}^2(t)}\\,, \\quad \\mathrm{where}\\quad \\tilde{\\eta}(t) =  \\eta_{\\mathrm{min}} \\Big(\\frac{\\eta_{\\mathrm{max}}}{\\eta_{\\mathrm{min}}}\\Big)^{t/T}.$$\n",
    "  \n",
    "2. (simulation-free) To get samples $x\\sim \\tilde{p}(\\cdot,t|x_0)$, we can simply use \n",
    "$$x= x_0 + \\tilde{\\eta}(t)z, \\quad \\mathrm{where} \\,,z\\sim \\mathcal{N}(0,\\mathrm{I}_d).$$\n",
    "\n",
    "\n",
    "3. Choose $w(t) = \\tilde{\\eta}^2(t)$\n",
    "  \n",
    "We obtain the loss\n",
    "\n",
    "\\begin{equation}\n",
    "      \\begin{aligned}\n",
    "\t\\mathrm{Loss}(u) =&  \n",
    "\\mathbb{E}_{t\\sim U([0,T])} \\mathbb{E}_{x_0\\sim p_0} \\mathbb{E}_{x\\sim\n",
    "\t\\tilde{p}(\\cdot,t|x_0)}\\Big[ \\Big(\\frac{1}{2} |u(x,t)|^2 - u(x,t) \\cdot \\nabla \\ln \\tilde{p}(x,t|x_0)\\Big) w(t)\\Big] \\\\\n",
    "\t=& \\mathbb{E}_{t\\sim U([0,T])}\n",
    "     \\mathbb{E}_{x_0\\sim p_0} \\mathbb{E}_{x\\sim \\tilde{p}(\\cdot,t|x_0)}\n",
    "     \\bigg[\\Big(\\frac{1}{2} |u(x,t)|^2 +u(x,t) \\cdot \\frac{x-\n",
    "     x_0}{\\tilde{\\eta}(t)^2}\\Big) \\tilde{\\eta}^2(t)\\bigg]\\\\\n",
    "\t=& \\mathbb{E}_{t\\sim U([0,T])}\n",
    "\t\\mathbb{E}_{x_0\\sim p_0} \\mathbb{E}_{z\\sim \\mathcal{N}(0,\\mathrm{I}_d)} \\bigg[\\Big(\\frac{1}{2} |u(x,t)|^2 + \\frac{u(x,t) \\cdot z}{\\tilde{\\eta}(t)}\\Big) \\tilde{\\eta}^2(t)\\bigg]\n",
    "      \\end{aligned}\n",
    "\\end{equation}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch-size\n",
    "batch_size = 2000\n",
    "\n",
    "# total training epochs\n",
    "total_epochs = 5000\n",
    "\n",
    "# Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define a dataloader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(total_epochs):   # for each epoch\n",
    "    \n",
    "    for idx, data in enumerate(data_loader):  # loop over all mini-batches \n",
    "        \n",
    "        # for each state in mini-batch, uniformaly sample time on [0,T]\n",
    "        t = torch.rand(data.shape[0]) * T \n",
    "        \n",
    "        # standard deviation of the Gaussian density at time t\n",
    "        std_t = sde.marginal_std(data, t)    \n",
    "        \n",
    "        # generate standard Gaussian random variables\n",
    "        z = torch.randn_like(data) \n",
    "        \n",
    "        # get samples distributed according to the Gaussian density at time t\n",
    "        xt = data + std_t.reshape(-1, 1) * z\n",
    "        \n",
    "        # evaluate the model\n",
    "        score = model(xt, t.reshape(-1,1)) \n",
    "\n",
    "        loss = torch.mean((0.5*torch.sum(score**2, dim=1) + torch.sum(score * z, dim=1) / std_t)*std_t**2)\n",
    "                        \n",
    "        optimizer.zero_grad()\n",
    "        # gradient step\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx == 0:\n",
    "            # record the loss    \n",
    "            loss_list.append(loss.item())  \n",
    "            if epoch % 200 == 0:\n",
    "                print ('epoch=%d\\n   loss=%.4f' % (epoch, loss.item()))   \n",
    "                \n",
    "fig, ax = plt.subplots(1,1, figsize=(5, 4))\n",
    "\n",
    "ax.plot(loss_list)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_title('loss vs epoch')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74df85",
   "metadata": {},
   "source": [
    "### display the (trained) score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2, 2, 100)\n",
    "t = torch.linspace(0, T, 100)\n",
    "xv, tv = torch.meshgrid(x, t, indexing='ij')\n",
    "\n",
    "score_xt = model(xv.reshape(-1, 1), tv.reshape(-1,1)).reshape(100,100)\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "im = ax.pcolormesh(xv.numpy(), tv.numpy(), score_xt.detach().numpy(), cmap='coolwarm',shading='auto')\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=1.0)\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "ax.set_xlabel(r'x',fontsize=20)\n",
    "ax.set_ylabel(r't',fontsize=20)\n",
    "ax.set_title('score function',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e15a67",
   "metadata": {},
   "source": [
    "### After learning the score function, we can generate new samples by simulating the backward process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe001f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell PyTorch to disable auto-differentiation            \n",
    "with torch.no_grad():\n",
    "    \n",
    "    # generate samples from prior \n",
    "    X = sde.prior(10000)\n",
    "    # sample backward process with learned score fuunction\n",
    "    backward_traj_set = sde.backward_sampling(X, model, N=N)\n",
    "    \n",
    "print (\"shape of the backward trajectory tensor:\", backward_traj_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087db181",
   "metadata": {},
   "source": [
    "### Let's compare the densities between the forward and backward processes at different time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2, figsize=(13, 5))\n",
    "\n",
    "for i, t in enumerate(t_list):\n",
    "    t_idx = int (t/dt)\n",
    "    sns.kdeplot(forward_traj_set[t_idx,:,:].flatten(), ax=ax[0], label='t=%.2f,forward' % t, bw_adjust=0.2, linestyle=\"-\", c=color_list[i])\n",
    "    b_t_idx = int ((T-t)/dt)\n",
    "    sns.kdeplot(backward_traj_set[b_t_idx,:, :].flatten(), label='t=%.2f,backward' % t, ax=ax[0], bw_adjust=0.2, linestyle=\"--\", c=color_list[i])\n",
    "\n",
    "ax[0].set_xlim(-2, 2)\n",
    "ax[0].legend()\n",
    "\n",
    "sns.kdeplot(forward_traj_set[0,:, :].flatten(), ax=ax[1], linestyle=\"-\", bw_adjust=0.2, c='b', label='truth')\n",
    "sns.kdeplot(backward_traj_set[-1,:,:].flatten(), ax=ax[1], linestyle=\"--\", bw_adjust=0.2, c='b', label='generated')\n",
    "ax[1].set_xlim(-2, 2)\n",
    "ax[1].legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
