{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26328c42",
   "metadata": {},
   "source": [
    "# Normalizing flows with Real NVP model\n",
    "\n",
    "\n",
    "We use the package **normflows** which implements normalizing flow models\n",
    "\n",
    "### github: \n",
    "    https://github.com/VincentStimper/normalizing-flows\n",
    "\n",
    "### install:\n",
    "\n",
    "```\n",
    "pip install normflows\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This notebook is based on [real_nvp_colab.ipynb](https://github.com/VincentStimper/normalizing-flows/blob/master/examples/real_nvp_colab.ipynb) in the **normflows** package, which you can run direclty on [colab](https://colab.research.google.com/github/VincentStimper/normalizing-flows/blob/master/examples/real_nvp_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d28a9e2",
   "metadata": {},
   "source": [
    "## Summary of the theory\n",
    "\n",
    "**Idea**: Express data $x\\in \\mathbb{R}^d$ as a transformation of (Gaussian) $z\\sim p_z(z)$.\n",
    "\n",
    "**Change of variable formula**\n",
    "\n",
    "Let $z\\sim p_z(z)$ and $x=f(z)$, where $f: \\mathbb{R}^d\\rightarrow \\mathbb{R}^d$ is invertible and differentiable. Then,  \n",
    "$$\n",
    "  p_x(x) = p_z(f^{-1}(x)) |\\det J_{f^{-1}}(x)|\\,.\n",
    "$$\n",
    "\n",
    "**Setup**:\n",
    "1. target $p^*_x(x)$ \n",
    "2. transformation $x=f(z;\\theta)$. \n",
    "\n",
    "**Goal**: find $\\theta$ such that $p_x(x;\\theta)$ is close to $p^*_x(x)$.\n",
    "  \n",
    "  \n",
    "**(forward) KL divergence**\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "    & D_{KL}\\Big(p^*_x(x)~|~p_x(x;\\theta)\\Big) \\\\\n",
    "    =& \\mathbb{E}_{x\\sim p^*_x(x)} \\Big(\\log \\frac{p^*_x(x)}{p_x(x;\\theta)}\\Big) \\\\\n",
    "    =& -\\mathbb{E}_{x\\sim p^*_x(x)} \\Big[\\log p_z(f^{-1}(x;\\theta)) + \\log |\\det J_{f^{-1}}(x;\\theta)|\\Big] + C\\,.\n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "**Loss function**\n",
    "\n",
    "  $$\\mathrm{Loss}(\\theta) = -\\frac{1}{N}\\sum_{n=1}^N \\Big[\\log p_z(f^{-1}(x_n;\\theta)) + \\log |\\det J_{f^{-1}}(x_n;\\theta)|\\Big]\\,.\n",
    "  $$\n",
    "  \n",
    "**Real NVP**\n",
    "\n",
    "$x=(x_1, x_2) = f(z)$ is defined as \n",
    "\n",
    "\\begin{equation*}\n",
    "      \\begin{aligned}\n",
    "      x_1 =& z_1\\,, \\\\\n",
    "\tx_2 =& \\exp(\\sigma_\\theta(z_1)) \\odot z_2 + \\mu_\\theta(z_1)\\,, \n",
    "      \\end{aligned}\n",
    "\\end{equation*}\n",
    "where $\\odot$ denotes elementwise product, and $\\sigma_\\theta, \\mu_\\theta: \\mathbb{R}^{d'}\\rightarrow \\mathbb{R}^{d-d'}$. The inverse of $f$ is \n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{aligned}\n",
    "  z_1 =& x_1\\,, \\\\\n",
    "z_2 =& \\exp(-\\sigma_\\theta(x_1)) \\odot (x_2 - \\mu_\\theta(x_1))\\,, \n",
    "  \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "The Jacobian determinant is $\\det J_{f^{-1}}(x) = \\exp(-\\sum_{j=1}^{d-d'}(\\sigma_\\theta(x_1))_j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07799cf5",
   "metadata": {},
   "source": [
    "### set up the normalizing flow model\n",
    "\n",
    "The num_layers is set to 32 in the original notebook. \n",
    "\n",
    "Run this notebook with:\n",
    "\n",
    "1. num_layers = 2\n",
    "2. num_layers = 32\n",
    "\n",
    "Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cc52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import normflows as nf\n",
    "\n",
    "# Define 2D Gaussian base distribution\n",
    "base = nf.distributions.base.DiagGaussian(2)\n",
    "\n",
    "# Define list of flows\n",
    "num_layers = 2\n",
    "flows = []\n",
    "for i in range(num_layers):\n",
    "    # Neural network with two hidden layers having 64 units each\n",
    "    # Last layer is initialized by zeros making training more stable\n",
    "    param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True)\n",
    "    # Add flow layer\n",
    "    flows.append(nf.flows.AffineCouplingBlock(param_map))\n",
    "    # Swap dimensions\n",
    "    flows.append(nf.flows.Permute(2, mode='swap'))\n",
    "    \n",
    "# Construct flow model\n",
    "model = nf.NormalizingFlow(base, flows)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print ('number of parameters: %d' % params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c1e63",
   "metadata": {},
   "source": [
    "### display the flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b203441",
   "metadata": {},
   "source": [
    "### display target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target distribution\n",
    "target = nf.distributions.TwoMoons()\n",
    "\n",
    "# make a grid\n",
    "grid_size = 200\n",
    "xx, yy = torch.meshgrid(torch.linspace(-3, 3, grid_size), torch.linspace(-3, 3, grid_size))\n",
    "# get grid points\n",
    "zz = torch.stack((xx, yy), dim=2).reshape(-1, 2)              \n",
    "\n",
    "# compute the log of density at points zz\n",
    "log_prob = target.log_prob(zz).reshape(xx.shape[0], xx.shape[1])\n",
    "# compute density from its log\n",
    "prob = torch.exp(log_prob)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pcolormesh(xx, yy, prob.numpy(), cmap='coolwarm')\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dd193",
   "metadata": {},
   "source": [
    "### Plot initial flow distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of the density at points zz\n",
    "log_prob = model.log_prob(zz).reshape(xx.shape[0], xx.shape[1])\n",
    "# compute density from its log\n",
    "prob = torch.exp(log_prob)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pcolormesh(xx, yy, prob.detach().numpy(), cmap='coolwarm')\n",
    "plt.gca().set_aspect('equal', 'box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51056c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "max_iter = 4000\n",
    "num_samples = 512\n",
    "show_iter = 500\n",
    "\n",
    "loss_hist = np.array([])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "for it in tqdm(range(max_iter)):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get training samples\n",
    "    x = target.sample(num_samples)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = model.forward_kld(x)\n",
    "    \n",
    "    # Do backprop and optimizer step\n",
    "    if ~(torch.isnan(loss) | torch.isinf(loss)):\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Log loss\n",
    "    loss_hist = np.append(loss_hist, loss.item())\n",
    "    \n",
    "    # Plot learned distribution\n",
    "    if (it + 1) % show_iter == 0:\n",
    "        # compute the log of density\n",
    "        log_prob = model.log_prob(zz)\n",
    "        # compute density from its log\n",
    "        prob = torch.exp(log_prob.reshape(xx.shape[0], xx.shape[1]))\n",
    "        prob[torch.isnan(prob)] = 0\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.pcolormesh(xx, yy, prob.detach().numpy(), cmap='coolwarm')\n",
    "        plt.gca().set_aspect('equal', 'box')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de31d57",
   "metadata": {},
   "source": [
    "### plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(loss_hist, label='loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d90e1",
   "metadata": {},
   "source": [
    "### compare the generated density with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ca65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target distribution\n",
    "f, ax = plt.subplots(1, 2, sharey=True, figsize=(11, 5))\n",
    "\n",
    "log_prob = target.log_prob(zz).reshape(xx.shape[0], xx.shape[1])\n",
    "prob = torch.exp(log_prob)\n",
    "\n",
    "ax[0].pcolormesh(xx, yy, prob.detach().numpy(), cmap='coolwarm')\n",
    "ax[0].set_aspect('equal', 'box')\n",
    "ax[0].set_axis_off()\n",
    "ax[0].set_title('Target', fontsize=24)\n",
    "\n",
    "# Plot learned distribution\n",
    "log_prob = model.log_prob(zz).reshape(xx.shape[0], xx.shape[1])\n",
    "prob = torch.exp(log_prob)\n",
    "prob[torch.isnan(prob)] = 0\n",
    "ax[1].pcolormesh(xx, yy, prob.detach().numpy(), cmap='coolwarm')\n",
    "\n",
    "ax[1].set_aspect('equal', 'box')\n",
    "ax[1].set_axis_off()\n",
    "ax[1].set_title('Real NVP', fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8262c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
