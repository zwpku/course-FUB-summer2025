{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26328c42",
   "metadata": {},
   "source": [
    "# Denoising diffusion probabilistic models (DDPMs)\n",
    "\n",
    "\n",
    "## Summary of the theory \n",
    "\n",
    "#### Markov chains\n",
    "\n",
    "1. forward: $q(x^{(1:N)}\\,|\\,x^{(0)}) = \\prod_{k=1}^{N} q(x^{(k)}\\,|\\,x^{(k-1)})$, where \n",
    "$$q(x^{(k)}\\,|\\,x^{(k-1)}) = \\mathcal{N}(x^{(k)}; \\sqrt{1-\\beta_k}x^{(k-1)}, \\beta_k \\mathrm{I}_d)$$\n",
    "\n",
    "2. reverse: $p_\\theta(x^{(0:N)}) = p(x^{(N)}) \\prod_{k=1}^{N} p_\\theta(x^{(k-1)}\\,|\\,x^{(k)})$, where \n",
    " \n",
    "\\begin{equation*}\n",
    "  p_\\theta(x^{(k-1)}\\,|\\,x^{(k)}) = \\mathcal{N}\\Big(x^{(k-1)}; \\mathrm{\\mu}_\\theta(x^{(k)}, k), \\sigma_k^2\\mathrm{I}_d\\Big)\\,.\n",
    "\\end{equation*}\n",
    "\n",
    "3. generated density: $p_\\theta(x^{(0)})= \\int p_\\theta(x^{(0:N)})\\, dx^{(1:N)}$.\n",
    "\n",
    "#### Upper bound of nagative log-likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    &\\mathbb{E}_{q_0}\\big(-\\log p_\\theta(x^{(0)})\\big) \\\\\n",
    "    = & \\mathbb{E}_{q_0}\\Big[-\\log \\Big(\\int p_\\theta(x^{(0:N)})\\, dx^{(1:N)}\\Big)\\Big]  \\\\\n",
    "    =& \\mathbb{E}_{q_0}\\Big[-\\log \\Big(\\int \\frac{p_\\theta(x^{(0:N)})}{q(x^{(1:N)}\\,|\\,x^{(0)})} q(x^{(1:N)}\\,|\\,x^{(0)})\\, dx^{(1:N)}\\Big)\\Big] \\\\\n",
    "    \\le & \\mathbb{E}_{q_0}\\Big[\\int -\\log\\Big(\\frac{p_\\theta(x^{(0:N)})}{q(x^{(1:N)}\\,|\\,x^{(0)})}\\Big) q(x^{(1:N)}\\,|\\,x^{(0)})\\, dx^{(1:N)}\\Big] \\\\\n",
    "    =& \\mathbb{E}_{\\mathbb{Q}}\\Big( -\\log \\frac{p_\\theta(x^{(0:N)})}{q(x^{(1:N)}\\,|\\,x^{(0)})}\\Big)\\\\\n",
    "    =& \\mathbb{E}_{\\mathbb{Q}}\\Big( -\\log p(x^{(N)}) - \\sum_{k=1}^{N}\n",
    "    \\log\\frac{p_\\theta(x^{(k-1)}\\,|\\,x^{(k)})}{q(x^{(k)}\\,|\\,x^{(k-1)})}\\Big)=:L \\,,\n",
    "  \\end{aligned}\n",
    "  \\label{variational-bound}\n",
    "\\end{equation}\n",
    "\n",
    "#### Idea\n",
    "\n",
    " optimize $\\theta$, or $\\mathrm{\\mu}_\\theta(x^{(k)},k)$, by minimizing $L$.\n",
    "\n",
    "#### Parametrization\n",
    "\n",
    "The derivation in the lecture note suggests the parametrization\n",
    "\n",
    "$$\\mathrm{\\mu}_\\theta(x^{(k)}, k) = \\frac{1}{\\sqrt{\\alpha_k}} \\Big(x^{(k)}- \\frac{\\beta_k}{\\sqrt{1-\\bar{\\alpha}_k}} \\epsilon_\\theta(x^{(k)}, k)\\Big)$$\n",
    "where $\\epsilon_\\theta(x^{(k)}, k)$ is a function modeled by a neural network.\n",
    "\n",
    "\n",
    "#### Simplified Loss function\n",
    "\n",
    "   $$L_{\\mathrm{simple}}(\\theta) = \\mathbb{E}_{k, x^{(0)}, \\epsilon} \\Big[\\big|\\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_k}x^{(0)} + \\sqrt{1-\\bar{\\alpha}_k} \\epsilon, k) - \\epsilon\\big|^2 \\Big]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_circles, make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3be450",
   "metadata": {},
   "source": [
    "### Set paramters in DDPM\n",
    "\n",
    "1. $\\beta_k$,    which is used in $q(x^{(k)}\\,|\\,x^{(k-1)}) = \\mathcal{N}(x^{(k)}; \\sqrt{1-\\beta_k}x^{(k-1)}, \\beta_k \\mathrm{I}_d)$\n",
    "2.  $\\alpha_k = 1 - \\beta_k\\,,\\quad \\bar{\\alpha}_k = \\prod_{i=1}^k \\alpha_i$, which are constants in  \n",
    "\n",
    "    $q(x^{(k)}|x^{(0)}) = \\mathcal{N}(x^{(k)}; \\sqrt{\\bar{\\alpha}_k}x^{(0)}, (1-\\bar{\\alpha}_k) \\mathrm{I}_d)$.\n",
    "    \n",
    "3. $\\sigma_k^2 = \\beta_k$, which is used in \n",
    "\n",
    "    $p_\\theta(x^{(k-1)}\\,|\\,x^{(k)}) = \\mathcal{N}\\Big(x^{(k-1)}; \\mathrm{\\mu}_\\theta(x^{(k)}, k), \\sigma_k^2\\mathrm{I}_d\\Big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=100\n",
    "beta_min = 0.001\n",
    "beta_max = 0.02\n",
    "\n",
    "beta_k = torch.linspace(beta_min, beta_max, N)\n",
    "\n",
    "alpha_k = 1.0 - beta_k\n",
    "\n",
    "alpha_bar_k = torch.ones_like(alpha_k)\n",
    "for i in range(N):\n",
    "    if i == 0 :\n",
    "        alpha_bar_k[i] = alpha_k[i]\n",
    "    else :\n",
    "        alpha_bar_k[i] = alpha_bar_k[i-1] * alpha_k[i]\n",
    "\n",
    "sigma_k = torch.sqrt(beta_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d602cf7",
   "metadata": {},
   "source": [
    "#### display the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('beta_k=', beta_k, ', length=%d' % beta_k.shape)\n",
    "print ('alpha_k=', alpha_k, ', length=%d' % alpha_k.shape)\n",
    "print ('alpha_bar_k=', alpha_bar_k, ', length=%d' % alpha_bar_k.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfa961",
   "metadata": {},
   "source": [
    "### feedforward neural network to represent $\\epsilon(x,k):\\mathbb{R}^d \\times \\{1,\\dots, N\\} \\rightarrow \\mathbb{R}^d$\n",
    "\n",
    "The goal is to learn the mean $\\mathrm{\\mu}_\\theta(x^{(k)}, k)$ in the transition density \n",
    "$p_\\theta(x^{(k-1)}\\,|\\,x^{(k)}) = \\mathcal{N}\\Big(x^{(k-1)}; \\mathrm{\\mu}_\\theta(x^{(k)}, k), \\sigma_k^2\\mathrm{I}_d\\Big)$ of the reverse Markov chain.\n",
    "\n",
    "We use the parametrization\n",
    "\n",
    "$$\\mathrm{\\mu}_\\theta(x^{(k)}, k) = \\frac{1}{\\sqrt{\\alpha_k}} \\Big(x^{(k)}- \\frac{\\beta_k}{\\sqrt{1-\\bar{\\alpha}_k}} \\epsilon_\\theta(x^{(k)}, k)\\Big)$$\n",
    "where $\\epsilon_\\theta(x^{(k)}, k)$ is a function modeled by a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class noise_predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, N=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 1, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 100), \n",
    "            nn.Tanh(),                      \n",
    "            nn.Linear(100, 100),             \n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(100, 100), \n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(100, dim),             \n",
    "       )\n",
    "        self.N = N * 1.0\n",
    "        \n",
    "    \n",
    "    def forward(self, x, k):\n",
    "\n",
    "        # combine x and t into one tensor    \n",
    "        state = torch.cat((x, k/self.N), dim=1)\n",
    "        \n",
    "        # pass input to the network\n",
    "        output = self.net(state)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70c619",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Loss function:\n",
    "\n",
    "   $$L_{\\mathrm{simple}}(\\theta) = \\mathbb{E}_{k, x^{(0)}, \\epsilon} \\Big[\\big|\\epsilon_\\theta(\\sqrt{\\bar{\\alpha}_k}x^{(0)} + \\sqrt{1-\\bar{\\alpha}_k} \\epsilon, k) - \\epsilon\\big|^2 \\Big]$$\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, model, learning_rate=1e-3, batch_size=1000, total_epochs=1000):\n",
    "    \n",
    "    # determine dimension from training data\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    # Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # change the dataset to PyTorch tensor\n",
    "    dataset = torch.tensor(X, dtype=torch.float32).reshape(-1,dim)\n",
    "\n",
    "    # define a dataloader\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(total_epochs):   # for each epoch\n",
    "\n",
    "        for idx, data in enumerate(data_loader):  # loop over all mini-batches \n",
    "\n",
    "            # for each state in mini-batch, uniformaly sample an index from 1,..., N\n",
    "            k = torch.randint(low=1, high=N+1, size=(data.shape[0], 1))  \n",
    "\n",
    "            # noise (standard Gaussian random variables)\n",
    "            epsilon = torch.randn_like(data) \n",
    "            \n",
    "            # xk given x0=data\n",
    "            xk = torch.sqrt(alpha_bar_k[k-1]) * data + torch.sqrt(1-alpha_bar_k[k-1]) * epsilon\n",
    "\n",
    "            # evaluate the model\n",
    "            noise_pred = model(xk, k) \n",
    "\n",
    "            loss = torch.mean(torch.sum((noise_pred - epsilon)**2, dim=1)) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # gradient step\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx == 0:\n",
    "                # record the loss    \n",
    "                loss_list.append(loss.item())  \n",
    "                if epoch % 100 == 0:\n",
    "                    print ('epoch=%d\\n   loss=%.4f' % (epoch, loss.item()))   \n",
    "                    \n",
    "    return loss_list         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e1a11",
   "metadata": {},
   "source": [
    "### generate new samples by sampling the reverse Markov chain: \n",
    "\n",
    "$$\n",
    "  x^{(k-1)} \\sim p_\\theta(x^{(k-1)}\\,|\\,x^{(k)}) = \\mathcal{N}\\Big(x^{(k-1)};\n",
    "    \\mathrm{\\mu}_\\theta(x^{(k)}, k), \\sigma_k^2\\mathrm{I}_d\\Big)\\,, \\qquad k=N, \\dots, 1\\,.\n",
    "$$\n",
    "  where $\\sigma_k^2 = \\beta_k$ and\n",
    "  $$\\mathrm{\\mu}_\\theta(x^{(k)}, k) = \\frac{1}{\\sqrt{\\alpha_k}} \\Big(x^{(k)}- \\frac{\\beta_k}{\\sqrt{1-\\bar{\\alpha}_k}} \\epsilon_\\theta(x^{(k)}, k)\\Big)$$\n",
    "\n",
    "Therefore, \n",
    "  \n",
    "$$\\begin{aligned}\n",
    "x^{(k-1)} =& \\mathrm{\\mu}_\\theta(x^{(k)}, k) + \\sigma_k z \\\\\n",
    "= & \\frac{1}{\\sqrt{\\alpha_k}} \\Big(x^{(k)}-\n",
    "  \\frac{\\beta_k}{\\sqrt{1-\\bar{\\alpha}_k}} \\epsilon_\\theta(x^{(k)}, k)\\Big) +\n",
    "  \\sigma_k z\\,, \\quad ~\\mathrm{where}~ z \\sim \\mathcal{N}(0, \\mathrm{I}_d)\\,.\n",
    "\\end{aligned}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_sampling(model, M, dim):\n",
    "       \n",
    "    # sample from prior (standard Gaussian)\n",
    "    x = torch.randn(M*dim).reshape(M, dim)    \n",
    "    \n",
    "    for k in reversed(range(1,N+1)):\n",
    "        if k > 1 : \n",
    "            z = torch.randn(M*dim).reshape(M, dim)  \n",
    "        else :\n",
    "            z = torch.zeros(M, dim)\n",
    "        \n",
    "        tmp = beta_k[k-1] / torch.sqrt(1-alpha_bar_k[k-1]) * model(x, torch.ones(M,1) * k)\n",
    "        \n",
    "        mu_theta = 1.0 / torch.sqrt(alpha_k[k-1]) * (x - tmp)\n",
    "        \n",
    "        x = mu_theta + sigma_k[k-1] * z\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282d3f6",
   "metadata": {},
   "source": [
    "### prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb43031",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=10)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5, 4))\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set_title(\"dataset\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0319f7d",
   "metadata": {},
   "source": [
    "### define the neural network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = noise_predictor(dim=2, N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a63c06",
   "metadata": {},
   "source": [
    "### training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8784779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch-size\n",
    "batch_size = 1000\n",
    "\n",
    "# total training epochs\n",
    "total_epochs = 5000\n",
    "\n",
    "# training \n",
    "loss_list = training(X, model, learning_rate=1e-3, batch_size=batch_size, total_epochs=total_epochs)\n",
    "\n",
    "# plot the evolution of the loss function during training\n",
    "fig, ax = plt.subplots(1,1, figsize=(5, 4))\n",
    "ax.plot(loss_list)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_title('loss vs epoch')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e15a67",
   "metadata": {},
   "source": [
    "### generate new samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71144375",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gen = reverse_sampling(model, M=10000, dim=2)\n",
    "\n",
    "X_gen = X_gen.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f62b4c",
   "metadata": {},
   "source": [
    "### compare generated data with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(8, 4))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1])\n",
    "ax[0].set_title(\"data\")\n",
    "ax[1].scatter(X_gen[:, 0], X_gen[:, 1])\n",
    "ax[1].set_title(\"generated\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729a9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
