{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# fix the random seed\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# output should be:\n",
    "#    tensor([0.0290, 0.4019, 0.2598, 0.3666])\n",
    "print (torch.rand(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0aa3e9",
   "metadata": {},
   "source": [
    "## Part 1: Feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b8445",
   "metadata": {},
   "source": [
    "## a simple neural network \n",
    "\n",
    "PyTorch provides various types of layers to build NNs:\n",
    "    https://docs.pytorch.org/docs/stable/nn.html\n",
    "    \n",
    " In this example, the layers are:\n",
    " 1. input layer of width 1\n",
    " 2. first hidden layer of width 3\n",
    " 3. second hidden layer of width 4\n",
    " 4. output layer of width 1\n",
    " \n",
    "It defines a function $f$ from $\\mathbb{R}$ to $\\mathbb{R}$:\n",
    " $$f(x) = f_3(\\psi(f_2(\\psi(f_1(x))))),$$\n",
    " where $\\psi$ is a (nonlinear) activation function, and\n",
    " 1. $f_1: \\mathbb{R}\\rightarrow \\mathbb{R}^3$\n",
    " 2. $f_2: \\mathbb{R}^3\\rightarrow \\mathbb{R}^4$\n",
    " 3. $f_3: \\mathbb{R}^4\\rightarrow \\mathbb{R}$\n",
    " \n",
    " are linear transformations.\n",
    " \n",
    " We have:\n",
    " $$\n",
    " f_i(x) = W_ix + b_i\n",
    " $$\n",
    " where $W_i$, $b_i$ are coefficients (parameters).\n",
    " \n",
    " **nn.Linear**: https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65819cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            # linear transformation\n",
    "            nn.Linear(1, 3),\n",
    "            # nonlinear activation \n",
    "            nn.Tanh(),\n",
    "            # notice that the width needs to match \n",
    "            nn.Linear(3, 4), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 1), \n",
    "            # you can include more layers here to make the network deeper!\n",
    "        )\n",
    "        \n",
    "    # define how the output of model is computed given input x\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "model = MyNet()\n",
    "\n",
    "# have a look at the model\n",
    "print(model)\n",
    "\n",
    "# print all (training) parameters of the model\n",
    "# These are the parameters $W_i, b_i$ in the linear functions $f_1, f_2, f_3$.\n",
    "for param in model.parameters():\n",
    "    print (param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db41a38",
   "metadata": {},
   "source": [
    "### we can evaluate the model on data\n",
    "\n",
    "we can evalue the model on multiple data points.\n",
    "\n",
    "**Note**: The last dimension of the input tensor x (dim=1) needs to match \n",
    "the input dimension of the network, while the first dimension of $x$ (usually the batch-size)\n",
    "can be any number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf118d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data, and change its shape to [N, dim], where\n",
    "#    N: number of points\n",
    "#  dim: 1\n",
    "x = torch.linspace(0, 1, 101).reshape(-1,1)\n",
    "\n",
    "print ('shape of x:', x.shape)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "# when plotting, we have to change the PyTorch tensor to numpy! \n",
    "plt.plot(x.detach().numpy(), y.detach().numpy(), c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffb547",
   "metadata": {},
   "source": [
    "### Let's make the network more general:\n",
    "\n",
    "User can specify width of the internal layers and activation function, such as: \n",
    "\n",
    "1. identity (by default): no nonlinear activation\n",
    "2. nn.Tanh() \n",
    "3. nn.ReLU() \n",
    "\n",
    "Many more are possible, see: https://docs.pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580907eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet1(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_width, activation=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        if activation is None:\n",
    "            act = nn.Identity()  \n",
    "        else:\n",
    "            act = activation  # use activation if provided\n",
    "            \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, layer_width),  # user-specified width \n",
    "            act,   # activation or identity\n",
    "            nn.Linear(layer_width, layer_width),\n",
    "            act,\n",
    "            nn.Linear(layer_width, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61eecfc",
   "metadata": {},
   "source": [
    "### examples \n",
    "\n",
    "We can define different networks by providing different parameters.\n",
    "\n",
    "Please notice the differences in the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190789d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# width is 3, no activation\n",
    "model = MyNet1(3)\n",
    "print(model)\n",
    "\n",
    "# width is 4, activation is tanh\n",
    "model = MyNet1(4, nn.Tanh())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50a592",
   "metadata": {},
   "source": [
    "## Part 2: Training\n",
    "\n",
    "#### We want to train neural networks to learn two functions on $x\\in [0,1]$:\n",
    "\n",
    "1. linear: $f_1(x)=3x+1$\n",
    "2. nonlinear: $f_2(x)=sin(\\pi x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8654877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first function is linear\n",
    "def linear_f(x):\n",
    "    return 3*x + 1\n",
    "\n",
    "# the second function is nonlinear\n",
    "def nonlinear_f(x):\n",
    "    return torch.sin(torch.pi * x)\n",
    "\n",
    "# evaluates functions on a grid of [0,1]\n",
    "x = torch.linspace(0, 1, 101).reshape(-1,1)\n",
    "linear_y = linear_f(x)\n",
    "sin_y = nonlinear_f(x)\n",
    "\n",
    "# plot these two functions\n",
    "plt.plot(x.detach().numpy(), linear_y.detach().numpy(), c='r', label='3x+1')\n",
    "plt.plot(x.detach().numpy(), sin_y.detach().numpy(), c='b', label='sin(pi x)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761526a",
   "metadata": {},
   "source": [
    "### we define the training procedure as a function so that we can reuse it.\n",
    "\n",
    "\n",
    "**mini-batch**: a (randomly drawn) subset of data used to evaluate the loss in each training step.\n",
    "\n",
    "**epoch**: consists of multiple training steps within which all data points have been drawn once. \n",
    "\n",
    "We use the **DataLoader** provided by PyTorch to sample mini-batch from dataset. \n",
    "\n",
    "We train the neural network for multiple epochs.  \n",
    "\n",
    "Users provides parameters:\n",
    "\n",
    "1. model: network to be trained.\n",
    "2. fun: the function to be learned (regression).\n",
    "3. batch_size: batch-size in training. \n",
    "4. total_epochs: number of training epochs.\n",
    "\n",
    "For DataLoader, see:\n",
    "1. https://docs.pytorch.org/docs/stable/data.html\n",
    "2. https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "For optimizer:\n",
    "\n",
    "see \n",
    "\n",
    "1. torch.optim: https://docs.pytorch.org/docs/stable/optim.html\n",
    "2. Adam: https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "3. Adam: https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, fun,  batch_size=10, total_epochs=10):\n",
    "    \n",
    "    # use mean square error (MSE) as loss function\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) \n",
    "    \n",
    "    # tell PyTorch that we want to optimize parameters in the model.\n",
    "    # we use Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    ## Input data set\n",
    "    x = torch.linspace(0, 1, 1001).reshape(-1,1)\n",
    "\n",
    "    ## grid used for plotting\n",
    "    small_grid = torch.linspace(0, 1, 50).reshape(-1,1)\n",
    "\n",
    "    # model before training\n",
    "    y_pred_untrained = model(small_grid)\n",
    "    \n",
    "    # specifiy the dataset and the batch-size\n",
    "    data_loader = DataLoader(x, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # list to record the loss\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in range(total_epochs):   # for each epoch\n",
    "        \n",
    "        for data in data_loader:  # loop over all mini-batches \n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing data to the model\n",
    "            y_pred = model(data)\n",
    "            # evaluate true function on mini-batch data \n",
    "            y = fun(data) \n",
    "\n",
    "            # Compute loss\n",
    "            \n",
    "            # Alternatively, we can write the loss function ourself:\n",
    "            #loss = ((y-y_pred)**2).sum()\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # gradient step\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "        # record the loss    \n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        #print(epoch, loss.item())\n",
    "          \n",
    "    \n",
    "    y = fun(small_grid)\n",
    "    y_pred = model(small_grid)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(10, 4))\n",
    "    \n",
    "    ax[0].plot(small_grid.detach().numpy(), y.detach().numpy(), '.', c='r', label='true')\n",
    "    ax[0].plot(small_grid.detach().numpy(), y_pred.detach().numpy(), '.', c='b', label='learned')\n",
    "    ax[0].plot(small_grid.detach().numpy(), y_pred_untrained.detach().numpy(), '.', c='gray', label='untrained')\n",
    "    ax[0].legend() \n",
    "    ax[0].set_xlabel('x')\n",
    "    \n",
    "    ax[1].plot(loss_list)\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].set_title('loss vs epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5ac6d",
   "metadata": {},
   "source": [
    "### Test 1: learn a linear function using a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1befd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no activation (hence linear model)\n",
    "model = MyNet1(3)\n",
    "print (model)\n",
    "training(model, linear_f, batch_size=100, total_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45beb3",
   "metadata": {},
   "source": [
    "### Test 2: learn a nonlinear function using a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffcfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet1(10)\n",
    "print (model)\n",
    "training(model, nonlinear_f, batch_size=100, total_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df384b1",
   "metadata": {},
   "source": [
    "### Test 3: Learn a nonlinear function using a nonlinear model \n",
    "\n",
    "(see how traning depends on model size and epochs.)\n",
    "\n",
    "#### Test 3.1: width=2, epoch=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08328a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet1(2, activation=nn.Tanh())\n",
    "print (model)\n",
    "training(model, nonlinear_f, batch_size=100, total_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1737039",
   "metadata": {},
   "source": [
    "#### Test 3.2: width=2, epoch=600 \n",
    "\n",
    "(more training epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet1(2, activation=nn.Tanh())\n",
    "print (model)\n",
    "training(model, nonlinear_f, batch_size=100, total_epochs=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd643cc",
   "metadata": {},
   "source": [
    "#### Test 3.3: width=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet1(10, activation=nn.Tanh())\n",
    "print (model)\n",
    "training(model, nonlinear_f, batch_size=100, total_epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afed26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
