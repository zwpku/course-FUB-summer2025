{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26328c42",
   "metadata": {},
   "source": [
    "# Flow-based generative model \n",
    "\n",
    "\n",
    "We use the package **torchdiffeq** for solving ODE. \n",
    "\n",
    "### github: \n",
    "    https://github.com/rtqichen/torchdiffeq\n",
    "\n",
    "### install:\n",
    "\n",
    "```\n",
    "pip install torchdiffeq\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfa961",
   "metadata": {},
   "source": [
    "### Define a simple feedforward neural network to model the vector field $u(x,t):\\mathbb{R}^d \\times [0,1] \\rightarrow \\mathbb{R}^d$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorField(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 1, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 100), \n",
    "            nn.Tanh(),                      \n",
    "            nn.Linear(100, 100),             \n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(100, 100), \n",
    "            nn.Tanh(),            \n",
    "            nn.Linear(100, dim),             \n",
    "       )\n",
    "        \n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        # combine x and t into one tensor    \n",
    "        state = torch.cat((x, t), dim=1)\n",
    "        \n",
    "        # pass input to the network\n",
    "        output = self.net(state)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59150c64",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "1. target density $p_1 = p_{\\mathrm{target}}$, \n",
    "2. prior density $p_0$. We choose standard Gaussian density.\n",
    "\n",
    "#### Linear interpolation: \n",
    "\n",
    "\\begin{equation}\n",
    "  X_t = (1-t) X_0 + tX_1 \\,, \\quad \\mathrm{where}~ X_0\\sim p_0 ~\\mathrm{and}~ X_1\\sim p_1\\,.\n",
    "\\end{equation}\n",
    "\n",
    "Let $p(\\cdot,t)$ be the probability density of $X_t$.\n",
    "\n",
    "**Idea**: learn an ODE \n",
    "\\begin{equation}\n",
    "  \\frac{dY_t}{dt} = u(Y_t,t)\\,, \\quad t\\in [0,1]\n",
    "\\end{equation}\n",
    "\n",
    "such that, when $Y_0\\sim p_0$, then $Y_t \\sim p(\\cdot, t)$ for any $t\\in[0,1]$.\n",
    "\n",
    "**Main theoretical result**\n",
    "\n",
    " The probability density $p(x,t)$ of $X_t$ solves the equation\n",
    " \\begin{equation*}\n",
    "  \\frac{\\partial p(x,t)}{\\partial t} + \\mathrm{div}\\Big(\\mathbb{E}\\big(X_1 - X_0\\big|X_t=x \\big) p(x,t)\\Big) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "  Therefore, we learn $u(x,t) = \\mathbb{E}\\big(X_1 - X_0|X_t=x \\big)$.\n",
    "  \n",
    "**Flow-matching loss**:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathrm{Loss}(u) =  \\mathbb{E}_{t\\sim U[0,1]} \\mathbb{E}_{X_0\\sim p_0,\n",
    "  X_1\\sim p_1}\\Big(\\big|u\\big((1-t)X_0+tX_1,t\\big) -  (X_1 - X_0)\\big|^2\\Big) \\,.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, model, learning_rate=1e-3, batch_size=1000, total_epochs=1000):\n",
    "    \n",
    "    # determine dimension from training data\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    # Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # change the dataset to PyTorch tensor\n",
    "    dataset = torch.tensor(X, dtype=torch.float32).reshape(-1,dim)\n",
    "\n",
    "    # define a dataloader\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(total_epochs):   # for each epoch\n",
    "\n",
    "        for idx, data in enumerate(data_loader):  # loop over all mini-batches \n",
    "\n",
    "            # for each state in mini-batch, uniformaly sample time on [0,1]\n",
    "            t = torch.rand(data.shape[0], 1)  \n",
    "\n",
    "            # generate standard Gaussian random variables\n",
    "            x0 = torch.randn_like(data) \n",
    "            \n",
    "            # compute linear interpolation\n",
    "            xt = (1-t) * x0 + t * data \n",
    "            \n",
    "            # evaluate the model\n",
    "            u = model(xt, t) \n",
    "\n",
    "            loss = torch.mean(torch.sum((u - (data - x0))**2, dim=1)) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # gradient step\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx == 0:\n",
    "                # record the loss    \n",
    "                loss_list.append(loss.item())  \n",
    "                if epoch % 100 == 0:\n",
    "                    print ('epoch=%d\\n   loss=%.4f' % (epoch, loss.item()))   \n",
    "                    \n",
    "    return loss_list         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e1a11",
   "metadata": {},
   "source": [
    "### generate new samples by solving the ODE:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{dY_t}{dt} = u(Y_t,t)\\,, \\quad t\\in [0,1]\n",
    "  \\label{ode-flow}\n",
    "\\end{equation}\n",
    "such that, when $Y_0\\sim p_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b0d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_ode(model, dim, N, t = torch.linspace(0,1,100)):\n",
    "    \n",
    "    # vector field of the ODE is learnt by training \n",
    "    def func(t,x):   \n",
    "        return model(x, torch.ones(x.shape[0], 1) * t)\n",
    "    \n",
    "    # sample y0 from prior (standard Gaussian)\n",
    "    y0 = torch.randn(N*dim).reshape(N, dim)    \n",
    "    \n",
    "    # solve ode using the solver from the package torchdiffeq\n",
    "    sol = odeint(func, y0, t)\n",
    "    \n",
    "    return t, sol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd14c4",
   "metadata": {},
   "source": [
    "## Example 1 ---- 1d dataset \n",
    "\n",
    "This dataset is the same as the one we studied for learning eigenfunctions and diffusion models\n",
    "\n",
    "We generate data by sampling a Brownian dynamics. \n",
    "\n",
    "### prepare dataset\n",
    "\n",
    "1. potential $V$\n",
    "2. its gradient\n",
    "3. sampling SDE\n",
    "4. set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential V, one-dimensional\n",
    "def V(x):\n",
    "    y1 = x**8\n",
    "    y2 = 0.8 * np.exp(-80 * x**2)\n",
    "    y3 = 0.55 * np.exp(-80 * (x-0.5)**2)\n",
    "    y4 = 0.3 * np.exp(-80 * (x+0.5)**2)\n",
    "\n",
    "    y = 2 * (y1 + y2 + y3 + y4)\n",
    "\n",
    "    return y\n",
    "\n",
    "# gradient of V\n",
    "def gradV(x):\n",
    "    y1 = 8 * x**7 \n",
    "    y2 = - 0.8 * 160 * x * np.exp(-80 * x**2)\n",
    "    y3 = - 0.55 * 160 * (x - 0.5) * np.exp(-80 * (x-0.5)**2) \n",
    "    y4 = - 0.3 * 160 * (x + 0.5) * np.exp(-80 * (x+0.5)**2)\n",
    "\n",
    "    y = 2 * (y1 + y2 + y3 + y4)\n",
    "\n",
    "    return y\n",
    "\n",
    "# sample the SDE using Euler-Maruyama scheme\n",
    "def sample(beta=1.0, dt=0.001, N=10000, seed=42):\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    X = 0.0\n",
    "    traj = []\n",
    "    tlist = []\n",
    "    for i in range(N):\n",
    "        traj.append(X)\n",
    "        tlist.append(dt*i)        \n",
    "        b = rng.normal()\n",
    "        X = X - gradV(X) * dt + np.sqrt(2 * dt/beta) * b\n",
    "\n",
    "    return np.array(tlist), np.array(traj)  \n",
    "\n",
    "# coefficient in SDE\n",
    "beta = 2.0\n",
    "# step-size \n",
    "dt = 0.005\n",
    "# number of sampling steps \n",
    "N = 10000\n",
    "# range of the domain \n",
    "xmin, xmax = -1.0, 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ff126",
   "metadata": {},
   "source": [
    "### 1.2 sample the SDE and display the trajectory \n",
    "\n",
    "**dataset** contains the training data we will use later.\n",
    "\n",
    "From the figure on the right, we see that our target density has 4 modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling SDE\n",
    "tvec, X = sample(beta, dt=dt, N=N)\n",
    "\n",
    "# show how many states are sampled\n",
    "print ('dataset has %d states.\\n' % X.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "# plot trajectory vs time\n",
    "ax.plot(tvec, X, alpha=0.7)\n",
    "ax.set_ylim([xmin, xmax])\n",
    "ax.set_xlabel(r'time')\n",
    "ax.set_ylabel(r'x')\n",
    "ax.set_title('trajectory')\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "# plot empirical density of the data\n",
    "ax1.hist(X, 50, density=True)\n",
    "\n",
    "ax1.set_title('impirical density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863b07e",
   "metadata": {},
   "source": [
    "### display neural network\n",
    "\n",
    "write it as a function, so that we can reuse it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vf(model):\n",
    "    x = torch.linspace(-2, 2, 100)\n",
    "    t = torch.linspace(0, 1, 100)\n",
    "    xv, tv = torch.meshgrid(x, t, indexing='ij')\n",
    "\n",
    "    score_xt = model(xv.reshape(-1, 1), tv.reshape(-1,1)).reshape(100,100)\n",
    "\n",
    "    fig = plt.figure(figsize=(5,4))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    im = ax.pcolormesh(xv.numpy(), tv.numpy(), score_xt.detach().numpy(), cmap='coolwarm',shading='auto')\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax, shrink=1.0)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "    ax.set_xlabel(r'x',fontsize=20)\n",
    "    ax.set_ylabel(r't',fontsize=20)\n",
    "    ax.set_title('vector field',fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de2253",
   "metadata": {},
   "source": [
    "### model the vector field by a neural network and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VectorField(dim=1)\n",
    "\n",
    "# For the moment, the neural network has not been trained.\n",
    "plot_vf(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fc291",
   "metadata": {},
   "source": [
    "### training the neural network by flow-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch-size\n",
    "batch_size = 1000\n",
    "\n",
    "# total training epochs\n",
    "total_epochs = 5000\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "# training \n",
    "loss_list = training(X, model, learning_rate=1e-3, batch_size=batch_size, total_epochs=total_epochs)\n",
    "\n",
    "# plot the evolution of the loss function during training\n",
    "fig, ax = plt.subplots(1,1, figsize=(5, 4))\n",
    "ax.plot(loss_list)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_title('loss vs epoch')    \n",
    "\n",
    "# plot the learned vector field\n",
    "plot_vf(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fed93a",
   "metadata": {},
   "source": [
    "### generate new samples by simulating the ODE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, sol = generative_ode(model, dim=1, N=10000)\n",
    "\n",
    "sol = sol.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7b612",
   "metadata": {},
   "source": [
    "### compare the distribution of generated samples and the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c20690",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(6, 5))\n",
    "\n",
    "sns.kdeplot(X[:, :].flatten(), ax=ax, linestyle=\"-\", bw_adjust=0.2, c='b', label='data distribution')\n",
    "sns.kdeplot(sol[-1,:,:].flatten(), ax=ax, linestyle=\"--\", bw_adjust=0.2, c='b', label='generated')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282d3f6",
   "metadata": {},
   "source": [
    "## Example 2 ----  2d dataset\n",
    "\n",
    "### prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb43031",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "X, Y = make_circles(\n",
    "    n_samples=n_samples, factor=0.5, noise=0.05, random_state=170\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5, 4))\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set_title(\"dataset\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0319f7d",
   "metadata": {},
   "source": [
    "### model the vector field by a neural network and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VectorField(dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a63c06",
   "metadata": {},
   "source": [
    "### training with flow-matching loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8784779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch-size\n",
    "batch_size = 1000\n",
    "\n",
    "# total training epochs\n",
    "total_epochs = 5000\n",
    "\n",
    "# training \n",
    "loss_list = training(X, model, learning_rate=1e-3, batch_size=batch_size, total_epochs=total_epochs)\n",
    "\n",
    "# plot the evolution of the loss function during training\n",
    "fig, ax = plt.subplots(1,1, figsize=(5, 4))\n",
    "ax.plot(loss_list)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_title('loss vs epoch')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e15a67",
   "metadata": {},
   "source": [
    "### generate new samples by simulating the ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71144375",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, sol = generative_ode(model, dim=2, N=10000)\n",
    "\n",
    "sol = sol.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.random.randn(X.shape[0] * 2).reshape(-1, 2)\n",
    "\n",
    "Xt = np.zeros([t.shape[0], X.shape[0], 2])\n",
    "for i, ti in enumerate(t):\n",
    "    Xt[i,:,:]= (1-ti) * X0 + ti * X\n",
    "    \n",
    "fig, ax = plt.subplots(2,4, figsize=(10, 3))\n",
    "\n",
    "ax[0,0].scatter(Xt[50, :, 0], Xt[50, :, 1])\n",
    "ax[0,0].set_title(\"t=0.5\")\n",
    "ax[1,0].scatter(sol[50, :, 0], sol[50, :, 1])\n",
    "ax[1,0].set_title(\"t=0.5\")\n",
    "\n",
    "ax[0,1].scatter(Xt[90, :, 0], Xt[90, :, 1])\n",
    "ax[0,1].set_title(\"t=0.9\")\n",
    "ax[1,1].scatter(sol[90, :, 0], sol[90, :, 1])\n",
    "ax[1,1].set_title(\"t=0.9\")\n",
    "\n",
    "ax[0,2].scatter(Xt[95, :, 0], Xt[95, :, 1])\n",
    "ax[0,2].set_title(\"t=0.95\")\n",
    "ax[1,2].scatter(sol[95, :, 0], sol[95, :, 1])\n",
    "ax[1,2].set_title(\"t=0.95\")\n",
    "\n",
    "\n",
    "ax[0,3].scatter(Xt[-1, :, 0], Xt[-1, :, 1])\n",
    "ax[0,3].set_title(\"t=1.0\")\n",
    "ax[1,3].scatter(sol[-1, :, 0], sol[-1, :, 1])\n",
    "ax[1,3].set_title(\"t=1.0\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8, 4))\n",
    "\n",
    "ax[0].scatter(X[:, 0], X[:, 1])\n",
    "ax[0].set_title(\"data\")\n",
    "ax[1].scatter(sol[-1, :, 0], sol[-1, :, 1])\n",
    "ax[1].set_title(\"generated\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e729a9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
