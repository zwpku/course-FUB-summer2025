{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085101d1",
   "metadata": {},
   "source": [
    "### Background:\n",
    "\n",
    "Brownian dynamics:\n",
    "\n",
    "\\begin{equation}\n",
    "  dX_t = -\\nabla V(X_t)\\,dt + \\sqrt{2\\beta^{-1}} dB_t\\,, \\quad t > 0\\,.\n",
    "\\end{equation}\n",
    "\n",
    "$X_t$ is ergodic with respect to a unique invariant density \n",
    "$$\\pi(x) = \\frac{1}{Z}\\mathrm{e}^{-\\beta V(x)},$$  \n",
    "where $$Z = \\int_{\\mathbb{R}} \\mathrm{e}^{-\\beta V(x)} dx$$ is a normalizing constant.\n",
    "\n",
    "The associated semigroup is  \n",
    "\\begin{equation}\n",
    "  (T_tg)(x) = \\mathbb{E}[g(X_t)|X_0=x], \\quad x\\in\\mathbb{R}^d\\,.\n",
    "\\end{equation}\n",
    "\n",
    "### This notebook illustrates how to solve the problem:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\nu_2 = \\max_{g\\in L^2_\\pi(\\mathbb{R}^d), \\langle g, \\mathbf{1}\\rangle_\\pi = 0} \\frac{\\langle T_\\tau g, g\\rangle_\\pi}{\\langle g, g\\rangle_\\pi} = \\max_{g\\in L^2_\\pi(\\mathbb{R}^d),\\, \\langle g,\\mathbf{1}\\rangle_\\pi = 0} \\frac{\\mathbb{E}_{X_0\\sim \\pi} \\big[g(X_0)g(X_\\tau)\\big]}{\\mathbb{E}_\\pi(g^2)},       \\qquad (*)\n",
    "\\end{equation}\n",
    "for fixed $\\tau > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89da87",
   "metadata": {},
   "source": [
    "## Part 1: define the system and generate data\n",
    "\n",
    "#### First, define the potential $V$ and its gradient\n",
    "\n",
    "The system is in 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential V, one-dimensional\n",
    "def V(x):\n",
    "    y1 = x**8\n",
    "    y2 = 0.8 * np.exp(-80 * x**2)\n",
    "    y3 = 0.55 * np.exp(-80 * (x-0.5)**2)\n",
    "    y4 = 0.3 * np.exp(-80 * (x+0.5)**2)\n",
    "\n",
    "    y = 2 * (y1 + y2 + y3 + y4)\n",
    "\n",
    "    return y\n",
    "\n",
    "# gradient of V\n",
    "def gradV(x):\n",
    "    y1 = 8 * x**7 \n",
    "    y2 = - 0.8 * 160 * x * np.exp(-80 * x**2)\n",
    "    y3 = - 0.55 * 160 * (x - 0.5) * np.exp(-80 * (x-0.5)**2)\n",
    "    y4 = - 0.3 * 160 * (x + 0.5) * np.exp(-80 * (x+0.5)**2)\n",
    "\n",
    "    y = 2 * (y1 + y2 + y3 + y4)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f550b",
   "metadata": {},
   "source": [
    "#### function to sample the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the SDE using Euler-Maruyama scheme\n",
    "def sample(beta=1.0, dt=0.001, N=10000, seed=42):\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    X = 0.0\n",
    "    traj = []\n",
    "    tlist = []\n",
    "    for i in range(N):\n",
    "        traj.append(X)\n",
    "        tlist.append(dt*i)        \n",
    "        b = rng.normal()\n",
    "        X = X - gradV(X) * dt + np.sqrt(2 * dt/beta) * b\n",
    "\n",
    "    return np.array(tlist), np.array(traj)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b644e5",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c023e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient in SDE\n",
    "beta = 2.0\n",
    "# step-size \n",
    "dt = 0.005\n",
    "# number of sampling steps \n",
    "N = 50000\n",
    "# range of the domain \n",
    "xmin, xmax = -1.0, 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb45ea3",
   "metadata": {},
   "source": [
    "### plot the potential $V(x)$ and the invariant density (boltzmann density)\n",
    "$$\\pi(x) = \\frac{1}{Z}\\mathrm{e}^{-\\beta V(x)},$$  \n",
    "where $$Z = \\int_{\\mathbb{R}} \\mathrm{e}^{-\\beta V(x)} dx$$ is a normalizing constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d688f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform grid on [xmin, xmax]\n",
    "xvec = np.linspace(xmin, xmax, 101)\n",
    "\n",
    "# potential on grid\n",
    "pot_vals = V(xvec)\n",
    "\n",
    "# compute invariant density\n",
    "density_unnormalized = np.exp(-beta * pot_vals)\n",
    "# normalizing constant Z\n",
    "z = np.sum(density_unnormalized) * (xmax-xmin) / 100\n",
    "# normalize to get densitz\n",
    "density_pi = density_unnormalized / z\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "# plot V\n",
    "ax.plot(xvec, pot_vals)\n",
    "ax.set_xlabel(r'x')\n",
    "ax.set_title(r'V')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "# plot invariant density\n",
    "ax.plot(xvec, density_pi)\n",
    "ax.set_xlabel(r'x')\n",
    "ax.set_title(r'invarint density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146745d9",
   "metadata": {},
   "source": [
    "### get trajectory data by sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399bf7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec, traj = sample(beta, dt=dt, N=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b1784c",
   "metadata": {},
   "source": [
    "### display the sampled trajectory and verify that the trajectory is long enough.\n",
    "\n",
    "When the simulation time is long enough, the empirical density of the trajectory data should match the invariant density (ergodic theorm). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc19a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "# plot trajectory vs time\n",
    "ax.plot(tvec, traj, alpha=0.5)\n",
    "ax.set_ylim([xmin, xmax])\n",
    "ax.set_xlabel(r'time')\n",
    "ax.set_ylabel(r'x')\n",
    "ax.set_title('trajectory')\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "# plot empirical density of the trajectory data\n",
    "ax1.hist(traj, 50, density=True, label='empirical density')\n",
    "\n",
    "# plot the invariant density\n",
    "ax1.plot(xvec, density_pi, label='invarint density')\n",
    "\n",
    "ax1.set_title('impirical and invariant density')\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733f459",
   "metadata": {},
   "source": [
    "# Part 2:   Marov state models (MSMs)\n",
    "\n",
    "\n",
    "### decomposition of the space $\\mathbb{R}$:\n",
    "\n",
    "1. $D_1=(-\\infty, xmin)$.\n",
    "\n",
    "2. The interval [xmin, xmax] is divided uniformly into $M'$ subsets, $D_2,\\dots, D_{M'+1}$.\n",
    "\n",
    "3. $D_{M'+2}=(xmax, +\\infty)$.\n",
    "\n",
    "% Therefore, there are $M=M'+2$ subsets in total.\n",
    "\n",
    "### function $g$ is approximated by piece-wise functions:\n",
    "\\begin{equation}\n",
    "  g(x)=\\sum_{i=1}^{M} \\omega_i \\mathbf{1}_{D_i}(x) \\,, \\quad\n",
    "  x\\in\\mathbb{R}\\,, \n",
    "\\end{equation}\n",
    "where $\\mathbf{1}_{D_i}(x)$ denotes the indicator function associated to the set $D_i$.\n",
    "\n",
    "### The problem (*) becomes \n",
    "\\begin{equation}\n",
    "  \\max_{\\omega \\in \\mathbb{R}^M, \\langle \\omega, \\mathbf{1}_M \\rangle_{\\hat{\\pi}} = 0} \\frac{\\langle \\hat{P} \\omega, \\omega \\rangle_{\\hat{\\pi}}}{\\langle \\omega, \\omega \\rangle_{\\hat{\\pi}}}\\,. \\qquad  (\\hat{*})\n",
    "\\end{equation}\n",
    "associated to the Markov chain defined by $\\hat{P}$.\n",
    "\n",
    "Notice that ($\\hat{*}$) is equivalent to solving the matrix eigenvalue problem\n",
    "\n",
    "$$\\hat{P}v=\\hat{\\nu} v.$$\n",
    "\n",
    "### expressions of the matrix $\\hat{P}$ and the invarint density $\\hat{\\pi}$ of the Markov chain \n",
    "\n",
    "$\\hat{\\pi}=(\\hat{\\pi}_1, \\dots, \\hat{\\pi}_{M})^\\top \\in \\mathbb{R}^{M}$, where \n",
    "  \\begin{equation}\n",
    "    \\hat{\\pi}_i = \\mathbb{E}_{x\\sim \\pi}\\big[\\mathbf{1}_{D_i}(x)\\big] = \\int_{\\mathbb{R}^d} \\mathbf{1}_{D_i}(x) \\pi(x)dx \\,.\n",
    "  \\end{equation}\n",
    "\n",
    "entries of the matrix $\\hat{P} \\in \\mathbb{R}^{M\\times M}$:\n",
    "\\begin{equation}\n",
    "  \\hat{P}_{ij} = \\frac{\\mathbb{E}_{X_0\\sim \\pi}\\big[\\mathbf{1}_{D_i}(X_0) \\mathbf{1}_{D_j}(X_\\tau)\\big]}{ \\mathbb{E}_{x\\sim\\pi}\n",
    "  \\big[\\mathbf{1}_{D_i}(x)\\big]}\\,,\\quad 1\\le i,j\\le M\\,.\n",
    "\\end{equation}\n",
    "\n",
    "### empirical expressions used in practice (by ergodic theorm):\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    \\hat{\\pi}_i =& \\int_{\\mathbb{R}^d} \\mathbf{1}_{D_i}(x) \\pi(x)dx = \\lim_{T\\rightarrow \\infty} \\frac{1}{T} \\int_0^ T \\mathbf{1}_{D_i}(X_t) dt \\approx \\frac{1}{N}\\sum_{n=0}^{N-1} \\mathbf{1}_{D_i}(X_n) \n",
    "  \\end{aligned}\n",
    "\\end{equation}  \n",
    "and\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}    \n",
    "    \\hat{P}_{ij} =& \\frac{\\mathbb{E}_{X_0\\sim \\pi}\\big[\\mathbf{1}_{D_i}(X_0) \\mathbf{1}_{D_j}(X_\\tau)\\big]}{ \\mathbb{E}_{x\\sim\\pi}\n",
    "    \\big[\\mathbf{1}_{D_i}(x)\\big]} = \\frac{\\lim_{T\\rightarrow \\infty}\n",
    "    \\frac{1}{T} \\int_0^ T \\mathbf{1}_{D_i}(X_t) \\mathbf{1}_{D_j}(X_{t+\\tau})\n",
    "    dt}{\\lim_{T\\rightarrow \\infty} \\frac{1}{T} \\int_0^ T\n",
    "    \\mathbf{1}_{D_i}(X_t) dt} \\approx \\frac{\\frac{1}{N-n'}\\sum_{n=0}^{N-n'-1} \\mathbf{1}_{D_i}(X_n) \\mathbf{1}_{D_j}(X_{n+n'})}{\\hat{\\pi}_i}     \n",
    "  \\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd37c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the index of the subset corresponding to state x\n",
    "\n",
    "def find_index(x, xmin, xmax, M_prime):\n",
    "    if x < xmin:  # 0 if x exceeds the lower range of [xmin, xmax] \n",
    "        return 0\n",
    "    if x > xmax:  # M+1 if x exceeds the upper range of [xmin, xmax] \n",
    "        return M_prime+1 \n",
    "    # the interval [xmin, xmax] is divided uniformly into M sub-intervals.\n",
    "    bin_width = (xmax-xmin) / M_prime\n",
    "    idx = int (np.floor((x - xmin) / bin_width)) + 1\n",
    "    return idx\n",
    "    \n",
    "M_prime = 200\n",
    "\n",
    "n_prime=10 \n",
    "tau = dt * n_prime\n",
    "\n",
    "# Total number of states is M+2 \n",
    "# initialize the vector and matrix \n",
    "pi_hat = np.zeros((M_prime+2))\n",
    "P = np.zeros((M_prime+2, M_prime+2))\n",
    "\n",
    "for i in range(N-n_prime):\n",
    "    x = traj[i]\n",
    "    idx1= find_index(x, xmin, xmax, M_prime)\n",
    "    # count the state\n",
    "    pi_hat[idx1] += 1\n",
    "    \n",
    "    x_next = traj[i+n_prime]    \n",
    "    idx2= find_index(x_next, xmin, xmax, M_prime)\n",
    "    # count the pair \n",
    "    P[idx1,idx2] += 1\n",
    "\n",
    "# normalize to get weights (densities)\n",
    "pi_hat /= np.sum(pi_hat)    \n",
    "\n",
    "# normalize to get entries of the probability matrix\n",
    "for i in range(M_prime+2):\n",
    "    if pi_hat[i] > 0:  \n",
    "        P[i, :] /= pi_hat[i] * (N-n_prime)\n",
    "\n",
    "print ('size of P:', P.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93392eca",
   "metadata": {},
   "source": [
    "### Solve the matrix eigenvalue problem\n",
    "\n",
    "$$\\hat{P}v=\\hat{\\nu} v.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the eigenvalue problem\n",
    "la, v = linalg.eig(P)\n",
    "\n",
    "# sort the eigenvalues so that the largest ones are ranked first. \n",
    "idx = la.real.argsort()[::-1]\n",
    "\n",
    "# get the real parts\n",
    "la = la.real[idx]\n",
    "v = v[:,idx].real\n",
    "\n",
    "print ('eigenvalues:\\n', la.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd105a81",
   "metadata": {},
   "source": [
    "### let's plot the approximation of the eigenfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbcee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "bins = np.arange(xmin, xmax, (xmax-xmin)/M_prime)\n",
    "# we comfine ourself to the range [xmin, xmax]\n",
    "eigvec = v[1:-1,:]\n",
    "\n",
    "# normalize to get eigenfunctions!\n",
    "for i in range(2):\n",
    "    scale = np.sqrt(np.sum(eigvec[:,i]**2*pi_hat[1:-1]))\n",
    "    eigvec[:,i] /= scale\n",
    "   \n",
    "for i in range(2):   \n",
    "    print ('eig %d: %.3f' % (i+1, la[i]))\n",
    "    ax.plot(bins, eigvec[:,i], alpha=0.5, label='eig_%d' % (i+1))\n",
    "\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_xlabel(r'x')\n",
    "ax.set_title('eigenfunctions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039aa784",
   "metadata": {},
   "source": [
    "# Part 2: Solving the problem using neural networks\n",
    "\n",
    "### change the optimization problem  (*) to an unconstrained optimization problem (by including penalties)\n",
    "\n",
    "\\begin{equation}\n",
    "  \\min_{g\\in \\Phi} \\Big[-\\frac{1}{\\tau}\\frac{\\langle T_\\tau g, g\\rangle_\\pi}{\\langle g, g\\rangle_\\pi} + \\alpha \\big(\\mathbb{E}_\\pi g\\big)^2 + \\alpha \\big(\\mathbb{E}_\\pi (g^2)-1\\big)^2\\Big]\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha > 0$ is a large penalty constant and we included a scaling constant $\\frac{1}{\\tau}$. The two penalty terms correspond to \n",
    "1. the constrant $\\langle g, \\mathbf{1}\\rangle_\\pi=0$.\n",
    "2. normalization: $\\langle g, g\\rangle_\\pi=1$, \n",
    "respectively.\n",
    "\n",
    "### Loss in practice:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathrm{Loss}(g)= \\min_{g\\in \\Phi} \\bigg[-\\frac{1}{\\tau}\\frac\n",
    "  {\\frac{1}{N-n'} \\sum_{n=0}^{N-n'-1} g(X_{n}) g(X_{n+n'})}{\n",
    "\\frac{1}{N} \\sum_{n=0}^{N-1} g^2(X_{n})} + \\alpha \\Big(\\frac{1}{N} \\sum_{n=0}^{N-1} g(X_{n})\\Big)^2 + \\alpha \\Big(\\frac{1}{N} \\sum_{n=0}^{N-1} g^2(X_{n})-1\\Big)^2 \\bigg]\\,.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch-size\n",
    "batch_size = 5000\n",
    "# penalty constant in the loss\n",
    "alpha = 10\n",
    "# total training epochs\n",
    "total_epochs = 300\n",
    "\n",
    "# represent the function g using a neural network\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(1, 20),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 1))\n",
    "# Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "\n",
    "# to evaluate loss, we need paired data at time t and t+tau \n",
    "# we build a dataloader that provides min-batch consisting of paired data\n",
    "x = torch.tensor(traj[:-n_prime], dtype=torch.float32).reshape(-1,1)\n",
    "x_tau = torch.tensor(traj[n_prime:], dtype=torch.float32).reshape(-1,1)\n",
    "traj_pair = torch.utils.data.TensorDataset(x, x_tau)\n",
    "\n",
    "data_loader = DataLoader(traj_pair, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# lists to record the loss, constraint, and approximation of eigenvalue \n",
    "loss_list = []\n",
    "constraint_list = []\n",
    "eig_list = []\n",
    "\n",
    "for epoch in range(total_epochs):   # for each epoch\n",
    "    \n",
    "    for idx, data in enumerate(data_loader):  # loop over all mini-batches \n",
    "        \n",
    "        # g(x(t))\n",
    "        y = model(data[0])\n",
    "        # g(x(t+tau))\n",
    "        y_tau = model(data[1])\n",
    "        # Rayleigh ratio in the maximization problem\n",
    "        eig_loss = (y*y_tau).mean() / (y**2).mean() \n",
    "        # objective consisting of Rayleigh ratio and penalties\n",
    "        loss = -1.0 / tau * eig_loss + alpha * (y.mean())**2 + alpha * ((y**2).mean()-1)**2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # gradient step\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if idx == 0:\n",
    "            # record the loss    \n",
    "            loss_list.append(loss.item())  \n",
    "            eig_list.append(eig_loss.item())\n",
    "            constraint_list.append(y.mean().item())\n",
    "            if epoch % 20 == 0:\n",
    "                print ('epoch=%d\\n   loss=%.4f, eig=%.4f, constraints=[%.3f, %.3f]' \\\n",
    "                       % (epoch, loss.item(), eig_loss.item(), y.mean(), (y**2).mean()))        \n",
    "    \n",
    "\n",
    "y = model(x)\n",
    "y_tau = model(x_tau)\n",
    "eig_val = (y*y_tau).mean() / (y**2).mean()\n",
    "\n",
    "print ('estimated eigenvalue: %.4f' % eig_val)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 4))\n",
    "\n",
    "ax[0].plot(loss_list)\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_title('loss vs epoch')\n",
    "\n",
    "ax[1].plot(constraint_list)\n",
    "ax[1].set_ylim([-0.5, 0.5])\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_title('constraint vs epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b6324",
   "metadata": {},
   "source": [
    "### Compare the solutions obtained using markov state models and neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5, 5))\n",
    "\n",
    "# evaluate and plot the neural network solution on grid\n",
    "y = model(torch.tensor(xvec, dtype=torch.float32).reshape(-1,1)) \n",
    "\n",
    "# may be necessary to change the sign \n",
    "#y *= -1.0\n",
    "\n",
    "ax.plot(xvec, y.detach().numpy(), '.', c='r', label='neural network')\n",
    "\n",
    "# plot the solution from Markov state models\n",
    "ax.plot(bins, eigvec[:,1], c='b', label='markov state model' )\n",
    "\n",
    "ax.legend() \n",
    "ax.set_xlabel('x')\n",
    "ax.set_title('eigenfunction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b534f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
